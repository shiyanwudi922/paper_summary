参考：

1、详解深度学习中的Normalization，不只是BN（1）：https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487494&idx=2&sn=e906da5b737193722f21f6def050a653&chksm=96e9cf86a19e46901eb34b1fcfac6f852859b40d1eead7054a8cbcba0c5a98be1e37e913b973&mpshare=1&scene=1&srcid=0207njq45mreDqX3cq2wzAWD&pass_ticket=Uzpva9k0ExVe31%2FKnOb2h0wbYcUdnqvBpmwCQEtyH%2F%2FjjRJlshI7Fv7tomZKBM%2B3#rd

2、详解深度学习中的Normalization，不只是BN（2）：https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487521&idx=2&sn=f8cece5a24ef3732e214f50a3ef7b854&chksm=96e9cfa1a19e46b7532a7ee865048840b7cfeabcf5cd4fb1d85e869b1bfc2aaabb89131f0928&mpshare=1&scene=1&srcid=0207fNPJUFy33BF8QcW4PlST&pass_ticket=8x50pwD7UJ7A3HV7LxswJ5PuJwLZfF1iweDIajjvjy5T40Uvjixe8IZx6qJhv4vE#rd



一、ICS（internal covariant shift）

1、表现：深度神经网络训练速度非常慢，或者难以收敛

2、表层原因：

（1）在每一轮迭代中，低层网络的参数都会发生变化，从而导致高层网络的输入数据分布在每次迭代时都不一样，该现象当网络层数增加时会更加明显。那么高层网络在训练过程中要不断地适应新的数据分布，从而使训练变慢；

（2）低层参数的变化，经过多层传输后，会使得高层网络的输入数据进入其饱和区，提前终止训练

3、深层原因：

（1）高层网络特征之间的“独立同分布”假设被破坏：经过多层网络传输之后，高层网络的输入特征之间已经不具有“独立同分布”的性质，从而降低了高层网络的性能。

（2）源空间和目标空间的数据分布不一致：即源空间和目标空间的条件概率是一致的，但是边缘概率不同。形式化定义为：对所有的x∈X，有Ps(Y|X=x) = Pt(Y|X=x)，但是Ps(X) != Pt(X)。

对应到神经网络中Ps(X)是低层网络的输入分布，Pt(X)是高层网络的输入分布，Y是最终的标签。经过多层传输后，低层和高层的输入分布会变的不同，且差异会随着网络深度的增加而增大，但是他们所指示的样本是不变的。即深度神经网络符合“源空间和目标空间数据分布不一致”的条件。

4、具体做法

h=f[g*((x-μ)/σ)+b]

即进行两次“先平移再放缩”的变换

5、第二次“平移放缩”的作用（g、b）：第一次平移放缩把数据变为标准分布，为什么又做一次平移放缩——保证模型的表达能力不会因为normalization而下降

（1）保留低层网络的表达能力：如果不做第二次平移放缩，那么无论低层的网络对数据做了什么操作，到高层时，都会被高层的normalization变成标准分布，相当于取消了低层的操作，导致模型的表达能力降低。而通过第二次平移放缩，且对应的参数可训练（g、b），使高层的特征可以变化到特定的一个分布上，从而保证了模型的表达能力。

（2）保留当前层的非线性表达能力：在高层网络中，通过一次平移放缩可以把输入数据变换到标准分布上，此时绝大部分数据都在几何函数的非饱和区，即只能利用激活函数的线性处理能力。而事实上，有些数据是需要处于饱和区的，通过区分饱和区和非饱和区，从而充分利用高层网络的非线性表示能力。加入第二次平移放缩后，能够使高层网络达到该目的。



二、四大主流normalization

1、batch normalization

特点：适用于每个 mini-batch 比较大，数据分布比较接近的场景。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。不适用于 动态的网络结构 和 RNN 网络。

2、layer normalization

特点：LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

3、weight normalization

特点：WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

4、cosine normalization

特点：解决了点积计算方式引起的“结果无界”的问题，通过用余弦计算代替内积计算实现了规范化。但是原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含二者的夹角信息，也包含两个向量的scale信息。如果去掉scale信息，可能导致表达能力的下降。



三、normalization有效的原因

1、权重的伸缩不变性：

Norm(W' * x) = Norm(W * x)，其中W' = λ * W

该性质对四种normalization的方式都成立

（1）权重伸缩不变性可以有效地提高反向传播的效率：因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。

（2）权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率：低层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。进而可以使用更高的学习率，提高学习速度。

2、数据的伸缩不变性：

Norm(W * x') = Norm(W * x)，其中x' = λ * x

该性质对BN、LN、CN成立，对WN不成立

（1）数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。





