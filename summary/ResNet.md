1、现象——degradation：当网络的深度到达一定程度后，继续增加网络的深度，训练和测试误差同时快速上升

2、原因猜测：

（1）梯度消逝/爆炸，导致网络的许多参数在层数增加以后不能得到更新：在实验中，已经使用了batch normalization，nomalized initialization，intermediate normalization layers和ReLU激励函数来抑制该问题；并且在实验中也验证了前向传输和反向传播的信号都没有消逝，所以不应该是这个原因；

（2）过拟合：在实验中，由于训练误差和测试误差同时上升，所以该问题也不是由于过拟合

（3）在网络的深度到达一定程度后，按照普通方式增加的非线性层对于从原始输入到最终输出的映射的拟合能力变弱：

考虑两个模型：A是一个浅层网络，B是在A的基础上增加多个非线性层形成的网络；只要B中增加的非线性层可以拟合恒等映射（即不做任何变换），则B的效果就不会比A差。而在实际试验中，B的效果是弱于A的，说明B中增加的层，其拟合能力变弱（对于恒等映射都很难拟合，更不用说对于更加复杂的映射）。

3、原因解释：

对于已经到达一定深度的网络而言，继续增加深度，有两个方面的效果：

（1）增加的网络层的拟合能力变差，从而使得模型总体的效果变差

（2）增加的网络层使得模型的特征表示能力增强，会提升模型的效果

（3）以上两个效果中，（1）占主导作用，所以会使总体的效果变差

4、改进方法：对于增加的每一个网络层，进行两路并行传输，其中一路进行恒等映射，保证网络维持原有的能力，另一路进行残差映射，通过对残差进行优化从而提升模型效果（对残差映射进行拟合比对原始映射进行拟合要容易很多）。

5、效果：

（1）一方面通过shotcut connection进行恒等映射，保证网络原有的能力；另一方面，通过对残差映射进行拟合（拟合残差映射比拟合原始映射函数更容易）来提升网络的性能。这两方面结合从而解决了degradation的问题

（2）网络的深度大幅度提升，表示能力提升，从而最终的准确率得到提升

（3）在网络深度增加的基础上，每一层的filter数量可以适当减少，从而整体的参数减少，训练速度更快

（4）最终的输出是原始输入与中间每一层的输出的和构成的，所以在反向传播时，梯度不再以乘积的形式出现，所以进一步解决了梯度消逝的问题

6、变形：

（1）在使用非常深的网络时，对原始的residual learning architecture进行修改，增加shorcut connection跨越的层数，从而转变成bottleneck architecture

![image](https://github.com/shiyanwudi922/paper_summary/blob/master/picture/ResNet/figure5.png)

（2）要点：

a、出于运行时间、节约存储计算资源的考虑

b、使用1*1的卷积进行先降维再升维

c、使用identity shortcut connection，不会增加计算复杂度和模型容量；如果用projection shortcut connection代替，由于其连接了两个维度非常高的层，所以会导致参数数量大量增加，计算复杂度和模型容量翻倍。

7、分析：

（1）在网络中，当某一层的残差为0时，实际上该层只剩下shotcut connection，说明整个网络中有效层的数量比真实数量少。

8、参考

（1）<https://zhuanlan.zhihu.com/p/22447440>

（2）<https://zhuanlan.zhihu.com/p/22071346>